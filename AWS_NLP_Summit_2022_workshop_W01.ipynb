{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS NLP Summit 2022 - Hands-on Workshop W01 Hugging Face Model Explainability for customer sentiment text analytics using SageMaker Clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. [Overview](#Overview)\n",
    " 1. [Prerequisites and Data](#Prerequisites-and-Data)\n",
    "     1. [Initialize SageMaker](#Initialize-SageMaker)\n",
    "     1. [Data preparation for model training](#Data-preparation-for-model-training) \n",
    " 1. [Train and Deploy Hugging Face Model](#Train-and-Deploy-Hugging-Face-Model)\n",
    "     1. [Train model with Hugging Face estimator](#Train-model-with-Hugging-Face-estimator)\n",
    "     1. [Deploy Model to Endpoint](#Deploy-Model)\n",
    " 1. [Model Explainability with SageMaker Clarify for text features](#Model-Explainability-with-SageMaker-Clarify-for-text-features)\n",
    "     1. [Explaining Predictions](#Explaining-Predictions)\n",
    "     1. [Visualize local explanations](#Visualize-local-explanations)\n",
    "     1. [Clean Up](#Clean-Up)\n",
    "\n",
    "## Overview\n",
    "In the workshop, we will use Amazon Review data set from Kaggle to train Hugging face mode, and predict customer sentiments, after use Amazon SageMaker Clarify to explain the hugging face models, which can help you understand which sections of text are most important for the predictions made by your model. This functionality can be used to explain an individual local prediction. You can define the length of the text segment (tokens, phrases, sentences, paragraphs) to understand and visualize a modelâ€™s behavior at multiple levels of granularity.\n",
    "\n",
    "Traning and test dataset from Kaggle\n",
    "https://www.kaggle.com/datasets/bittlingmayer/amazonreviews\n",
    "\n",
    "Pre-requirments. \n",
    "\n",
    "1. Sign-in Kaggle account, \n",
    "2. Download new API Token in your local and upload into your notebook.Please refer the doc. https://github.com/Kaggle/kaggle-api\n",
    "\n",
    "In doing so, the notebook will first train a [Hugging Face model](https://huggingface.co/models) using the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) in the SageMaker Python SDK using training dataset, then use SageMaker Clarify to analyze a testing dataset in CSV format, and then visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "1. Download kaggle.json from your Kaggle account, and upload into your notebook\n",
    "2. Create ~/.kaggle, move kaggle.json into ~/.kaggle\n",
    "3. change the access permissions;\n",
    "4. Download the dataset from Kaggle into your notebook;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ~/nlp-summit-workshop/kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d nabamitachakraborty/amazon-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy dataset to S3\n",
    "#Install AWS Cli\n",
    "#In order to upload the file to S3 we need to install AWS Cli.\n",
    "\n",
    "!pip install awscli --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the file\n",
    "from zipfile import ZipFile\n",
    "with ZipFile('amazon-reviews.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install botocore --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sagemaker botocore boto3 awscli --upgrade\n",
    "# Pls ingore error of pip's dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Data\n",
    "\n",
    "We require the following AWS resources to be able to successfully run this notebook.\n",
    "1. Kernel: Python 3 (Data Science) kernel on SageMaker Studio or `conda_python3` kernel on notebook instances\n",
    "2. Instance type: Any GPU instance. For hugging face model training, we use `ml.p3.2xlarge`, for hugging face model endpoint, we use `ml.g4dn.xlarge`\n",
    "3. [SageMaker Python SDK](https://pypi.org/project/sagemaker/) version 2.70.0 or greater\n",
    "4. [Transformers](https://pypi.org/project/transformers/) > 4.6.1\n",
    "5. [Datasets](https://pypi.org/project/datasets/) > 1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --quiet install \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" \"captum\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the latest version of the SageMaker Python SDK, boto, and AWS CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Initialize SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data loading and pre-processing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import botocore\n",
    "import sagemaker\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, clarify\n",
    "from captum.attr import visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# SageMaker session bucket is used to upload the dataset, model and model training logs\n",
    "sess = sagemaker.Session()\n",
    "sess = sagemaker.Session(default_bucket=sess.default_bucket())\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-sagemaker-clarify-text\"\n",
    "\n",
    "# Define the IAM role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# SageMaker Clarify model directory name\n",
    "model_path = \"model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you change the value of `model_path` variable above, please be sure to update the `model_path` in [`code/inference.py`](./code/inference.py) script as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data: Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", index_col=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(frac=0.80)\n",
    "df = df.sample(n=100000)\n",
    "# Original dataset has 3.6M data entries, for 100k entries, ml.p3.2xlarge training duration is 38min, for 1 hrs workshop, you can consider sampling 50k data instances for practic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally, 1 means negative label, 2 means positive label, we change to 0 and 1\n",
    "df.loc[df['Label'] == 1, 'Label'] = 0\n",
    "df.loc[df['Label'] == 2, 'Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common approach for model evaluation is using the train/validation/test split. Although this approach can be very effective in general, it can result in misleading results and potentially fail when used on classification problems with a severe class imbalance. Instead, the technique must be modified to stratify the sampling by the class label as below. Stratification ensures that all classes are well represented across the train, validation and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Label\"\n",
    "cols = \"Review\"\n",
    "\n",
    "X = df[cols]\n",
    "y = df[target]\n",
    "\n",
    "# Data split: 11%(val) of the 90% (train and test) of the dataset ~ 10%; resulting in 80:10:10split\n",
    "test_dataset_size = 0.10\n",
    "val_dataset_size = 0.11\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Stratified train-val-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_dataset_size, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_dataset_size, stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Dataset: train \",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    y_train.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: validation \",\n",
    "    X_val.shape,\n",
    "    y_val.shape,\n",
    "    y_val.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: test \",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    y_test.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "\n",
    "# Combine the independent columns with the label\n",
    "df_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "df_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "df_val = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split the dataset into train, test, and validation datasets. We use the train and validation datasets during training process, and run Clarify on the test dataset.\n",
    "\n",
    "In the cell below, we convert the Pandas DataFrames into Hugging Face Datasets for downstream modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload prepared dataset to the S3\n",
    "Here, we upload the prepared datasets to S3 buckets so that we can train the model with the Hugging Face Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 key prefix for the datasets\n",
    "s3_prefix = \"samples/datasets/amazon_reviews\"\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test\"\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step of the workflow, we use the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to load the pre-trained `distilbert-base-uncased` model and fine-tune the model on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with Hugging Face estimator\n",
    "The hyperparameters defined below are parameters that are passed to the custom PyTorch code in [`scripts/train.py`](./scripts/train.py). The only required parameter is `model_name`. The other parameters like `epoch`, `train_batch_size` all have default values which can be overriden by setting their values here.\n",
    "\n",
    "Training duration will be 40min\n",
    "(Original dataset has 3.6M data entries, when sampling 100k entries, ml.p3.2xlarge training duration is 38min, for 1 hrs workshop, you can consider sampling 50k data instances for practic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters passed into the training job\n",
    "hyperparameters = {\"epochs\": 1, \"model_name\": \"distilbert-base-uncased\"}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"scripts\",\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained model files for model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {huggingface_estimator.model_data} model.tar.gz\n",
    "! mkdir -p {model_path}\n",
    "! tar -xvf model.tar.gz -C  {model_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Deploy Model\n",
    "We are going to use the trained model files along with the PyTorch Inference container to deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"hf_model.tar.gz\", mode=\"w:gz\") as archive:\n",
    "    archive.add(model_path, recursive=True)\n",
    "    archive.add(\"code/\")\n",
    "prefix = s3_prefix.split(\"/\")[-1]\n",
    "zipped_model_path = sess.upload_data(path=\"hf_model.tar.gz\", key_prefix=prefix + \"/hf-model-sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"amazon-reviews-model-{}\".format(\n",
    "    datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    ")\n",
    "endpoint_name = \"amazon-reviews-endpoint-{}\".format(\n",
    "    datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(\n",
    "    entry_point=\"inference.py\",\n",
    "    name=model_name,\n",
    "    model_data=zipped_model_path,\n",
    "    role=get_execution_role(),\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    ")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.g4dn.xlarge\", endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model endpoint\n",
    "Lets test the model endpoint to ensure that deployment was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence1 = \"Good Functions But Bad Range: I bought this for my classroom and used it a few times. I loved the fact that it has a mouse function and a laser pointer(though it is a very weak laser). However, this product failed to deliver where it's needed the most and that is the range. I'm not sure why the last guy says it works from 40-50 feet but mine was only able to work within 10 feet. I also had to point my presenter directly at the USB device to work. At this range, I might as well walk to the computer to change a slide. Junk.\"\n",
    "test_sentence2 = \"Love the color! very soft. unique look. can't wait to wear it this fall\"\n",
    "test_sentence3 = (\n",
    "    \"Beautiful! This tape is great for anyone wanting some simple, effective self defence techniques for fighting larger attackers, or fighting in general. This tape is stuffed full of them! Kicks, stomps, neck manipulations, knees, elbows, punches, bites, head butts, improvised weapons, you name it! The best part is seeing Bob whoop a guy that is 8 inches taller and outweighs him by 120 pounds! The only thing I disagree with is the closed fist punches to the head (good chance of broken knuckles), and head butting without controlling the opponent's head (good chance of cutting your scalp!). But the good easily outweighs the bad.No one can argue with physics - size does matter. But Bob shows that the little guy still has a fighting chance. One of my favourite TRS tapes!\n",
    "\"\n",
    ")\n",
    "test_sentence4 = \"Poor plot with poor acting. I REALLY WANTED TO SEE THIS SINCE IT WAS AN ED WOOD MOVIE. I THOUGHT IT WAS RATHER ODD THE FIRST COUPLE OF MINUTES. WITH POOR DAY LIGHT TO NIGHT TIME INTERPRETATIONS, ORGY OF THE DEAD WAS DEFINITELY ONE OF THE LOWER FILMS ON THE LIST. IT HAS VERY POOR SCRIPT WRITING AND IT WAS SIMPLY DULL. YES THERE IS NUDITY BUT IT GETS OLD AS THE MOVIE DRAGS ON. IT WAS A VERY SLOW MOVING MOVIE. I REALLY WOULD CONSIDER TO PASS THIS ONE UP, HOWEVER IF YOU STILL HAVE THE SLIGHTEST POINT OF INTEREST GO AHEAD AND RENT IT.\n",
    "\"\n",
    "\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name, sess)\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.CSVDeserializer()\n",
    "predictor.predict([[test_sentence1], [test_sentence2], [test_sentence3], [test_sentence4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability with SageMaker Clarify for text features\n",
    "\n",
    "Now that the model is ready, and we are able to get predictions, we are ready to get explanations for text data from Clarify processing job. For a detailed example that showcases how to use the Clarify processing job, please refer to [this example](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.ipynb). This example shows how to get explanations for text data from Clarify.\n",
    "\n",
    "In the cell below, we create the CSV file to pass on to the Clarify dataset. We are using 10 samples here to make it fast, but we can use entire dataset at a time. We are also filtering out any reviews with less than 500 characters as long reviews provide better visualization with `sentence` level granularity (When granularity is `sentence`, each sentence is a feature, and we need a few sentences per review for good visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"clarify_data.csv\"\n",
    "num_examples = 10\n",
    "\n",
    "df_test[\"len\"] = df_test[\"Review\"].apply(lambda ele: len(ele))\n",
    "\n",
    "df_test_clarify = pd.DataFrame(\n",
    "    df_test[df_test[\"len\"] > 500].sample(n=num_examples, random_state=RANDOM_STATE),\n",
    "    columns=[\"Review\"],\n",
    ")\n",
    "df_test_clarify.to_csv(file_path, header=True, index=False)\n",
    "df_test_clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Predictions\n",
    "There are expanding business needs and legislative regulations that require explanations of _why_ a model made the decision it did. SageMaker Clarify uses SHAP to explain the contribution that each input feature makes to the final decision.\n",
    "\n",
    "How does the Kernel SHAP algorithm work? Kernel SHAP algorithm is a local explanation method. That is, it explains each instance or row of the dataset at a time. To explain each instance, it perturbs the features values - that is, it changes the values of some features to a baseline (or non-informative) value, and then get predictions from the model for the perturbed samples. It does this for a number of times per instance (determined by the optional parameter `num_samples` in `SHAPConfig`), and computes the importance of each feature based on how the model prediction changed.\n",
    "\n",
    "We are now extending this functionality to text data. In order to be able to explain text, we need the `TextConfig`. The `TextConfig` is an optional parameter of `SHAPConfig`, which you need to provide if you need explanations for the text features in your dataset. `TextConfig` in turn requires three parameters:\n",
    "1. `granularity` (required): To explain text features, Clarify further breaks down text into smaller text units, and considers each such text unit as a feature. The parameter `granularity` informs the level to which Clarify will break down the text: `token`, `sentence`, or `paragraph` are the allowed values for `granularity`.\n",
    "2. `language` (required): the language of the text features. This is required to tokenize the text to break them down to their granular form.\n",
    "3. `max_top_tokens` (optional): the number of top token attributions that will be shown in the output (we need this because the size of vocabulary can be very big). This is an optional parameter, and defaults to 50.\n",
    "\n",
    "Kernel SHAP algorithm requires a baseline (also known as background dataset). In case of tabular features, the baseline value/s for a feature is ideally a non-informative or least informative value for that feature. However, for text feature, the baseline values must be the value you want to replace the individual text feature (token, sentence or paragraph) with. For instance, in the example below, we have chosen the baseline values for `review` as `<XXX>`, and `granularity` is `sentence`. Every time a sentence has to replaced in the perturbed inputs, we will replace it with `<XXX>`.\n",
    "\n",
    "\n",
    "If baseline is not provided, a baseline is calculated automatically by SageMaker Clarify using K-means or K-prototypes in the input dataset for tabular features. For text features, if baseline is not provided, the default replacement value will be the string `<PAD>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, instance_count=1, instance_type=\"ml.m5.2xlarge\", sagemaker_session=sess\n",
    ")\n",
    "\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "explainability_output_path = \"s3://{}/{}/clarify-text-explainability\".format(bucket, prefix)\n",
    "explainability_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=file_path,\n",
    "    s3_output_path=explainability_output_path,\n",
    "    headers=[\"Review\"],\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_config = clarify.SHAPConfig(\n",
    "    baseline=[[\"<XXX>\"]],\n",
    "    num_samples=1000,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True,\n",
    "    text_config=clarify.TextConfig(granularity=\"sentence\", language=\"english\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the clarify explainability job involves spinning up a processing job and a model endpoint which may take a few minutes.\n",
    "# After this you will see a progress bar for the SHAP computation.\n",
    "# The size of the dataset (num_examples) and the num_samples for shap will effect the running time.\n",
    "# The duration is around 18min\n",
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize local explanations\n",
    "\n",
    "We use Captum to visualize the feature importances computed by Clarify.\n",
    "First, lets load the local explanations. Local text explanations can be found in the analysis results folder in a file named `out.jsonl` in the `explanations_shap` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_feature_attributions_file = \"out.jsonl\"\n",
    "analysis_results = []\n",
    "analysis_result = sagemaker.s3.S3Downloader.download(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file,\n",
    "    local_path=\"./\",\n",
    ")\n",
    "\n",
    "shap_out = []\n",
    "file = sagemaker.s3.S3Downloader.read_file(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file\n",
    ")\n",
    "for line in file.split(\"\\n\"):\n",
    "    if line:\n",
    "        shap_out.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local explanations file is a JSON Lines file, that contains the explanation of one instance per row. Let's examine the output format of the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(shap_out[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the highest level of this JSON Line, there are two keys: `explanations`, `join_source_value` (Not present here as we have not included a `joinsource` column in the input dataset). `explanations` contains a list of attributions for each feature in the dataset. In this case, we have a single element, because the input dataset also had a single feature. It also contains details like `feature_name`, `data_type` of the features (indicating whether Clarify inferred the column as numerical, categorical or text). Each token attribution also contains a `description` field that contains the token itself, and the starting index of the token in original input. This allows you to reconstruct the original sentence from the output as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, we create a list of attributions and a list of tokens for use in visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_dataset = [\n",
    "    np.array([attr[\"attribution\"][0] for attr in expl[\"explanations\"][0][\"attributions\"]])\n",
    "    for expl in shap_out\n",
    "]\n",
    "tokens_dataset = [\n",
    "    np.array(\n",
    "        [attr[\"description\"][\"partial_text\"] for attr in expl[\"explanations\"][0][\"attributions\"]]\n",
    "    )\n",
    "    for expl in shap_out\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain predictions as well so that they can be displayed alongside the feature attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict([t for t in df_test_clarify.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is a wrapper around the captum that helps produce visualizations for local explanations. It will\n",
    "# visualize the attributions for the tokens with red or green colors for negative and positive attributions.\n",
    "def visualization_record(\n",
    "    attributions,  # list of attributions for the tokens\n",
    "    text,  # list of tokens\n",
    "    pred,  # the prediction value obtained from the endpoint\n",
    "    delta,\n",
    "    true_label,  # the true label from the dataset\n",
    "    normalize=True,  # normalizes the attributions so that the max absolute value is 1. Yields stronger colors.\n",
    "    max_frac_to_show=0.05,  # what fraction of tokens to highlight, set to 1 for all.\n",
    "    match_to_pred=False,  # whether to limit highlights to red for negative predictions and green for positive ones.\n",
    "    # By enabling `match_to_pred` you show what tokens contribute to a high/low prediction not those that oppose it.\n",
    "):\n",
    "    if normalize:\n",
    "        attributions = attributions / max(max(attributions), max(-attributions))\n",
    "    if max_frac_to_show is not None and max_frac_to_show < 1:\n",
    "        num_show = int(max_frac_to_show * attributions.shape[0])\n",
    "        sal = attributions\n",
    "        if pred < 0.5:\n",
    "            sal = -sal\n",
    "        if not match_to_pred:\n",
    "            sal = np.abs(sal)\n",
    "        top_idxs = np.argsort(-sal)[:num_show]\n",
    "        mask = np.zeros_like(attributions)\n",
    "        mask[top_idxs] = 1\n",
    "        attributions = attributions * mask\n",
    "    return visualization.VisualizationDataRecord(\n",
    "        attributions,\n",
    "        pred,\n",
    "        int(pred > 0.5),\n",
    "        true_label,\n",
    "        attributions.sum() > 0,\n",
    "        attributions.sum(),\n",
    "        text,\n",
    "        delta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can customize the following display settings\n",
    "normalize = True\n",
    "max_frac_to_show = 1\n",
    "match_to_pred = False\n",
    "labels = test_dataset[\"Label\"][:num_examples]\n",
    "vis = []\n",
    "for attr, token, pred, label in zip(attributions_dataset, tokens_dataset, preds, labels):\n",
    "    vis.append(\n",
    "        visualization_record(\n",
    "            attr, token, float(pred[0]), 0.0, label, normalize, max_frac_to_show, match_to_pred\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we compiled the record we are finally ready to render the visualization.\n",
    "\n",
    "We see a row per review in the selected dataset. For each row we have the prediction, the label, and the highlighted text. Additionally, we show the total sum of attributions (as attribution score) and its label (as attribution label), which indicates whether it is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.05)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-1.23</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Bruce Springsteen Dust and Devils: I'm a Bruce fan.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> There's been a lot of media blitz for this project.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> But the fact remands it's not a great album.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> There is the title cut which I think is good and the song The Hitter.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I would rate the remanding tracks as fillers.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I was interested in the 5:1 and the dual disc technology as much as the album itself.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If you want a great Bruce album check out The River or Nebraska.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Maybe a live album of the tour to those who bought this album and spent their hard made money could save this project and face.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>1.30</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Excellent CD, Far Better Than Fanmail: Although usually I agree for the most part with Amazon's reviews, this one is totally off the mark.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I own both this CD and TLC's Fanmail, and while this was Destiny's Child put out an incredible disc, TLC released a one full of all hype and no talent.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> While TLC has become successful by their attitude rather than their singing or rapping, Destiny's Child has four very talented girls that show remarkable vocal ability for their age.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> DC seems to add a special touch to every song.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> While Bills(X3) perfectly conveys its message of sadness and betrayal, No Scrubs lacks no substance.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Alll of the songs are great, especially Bills(X3), Bug-A-Boo, Temptation, and Jumpin Jumpin.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> As for Beyonce stealing the show, she has a voice that compete with any of the young r&b divas, except for Monica.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The other three members have not complained about not having the leads, although all of them are capable of singing quite well.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-5.97</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> awful experience: This vacuum is awful.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The rotation stopped working, and was just putting dirt back in my carpet.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I have had 2 in the last 4 mos.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It gets clogged unless you change it every room.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I don't have pets, and It wouldn't filter down into the clear view canister.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> We had to change the belt in the first one which we had for 3 mos.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> That's not right.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The second one started smoking and burning up after 2 weeks.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It is not worth it.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Spend a little extra and get a good one.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I will never buy a Eurkea product again.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-1.48</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Good Functions But Bad Range: I bought this for my classroom and used it a few times.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I loved the fact that it has a mouse function and a laser pointer(though it is a very weak laser).                    </font></mark><mark style=\"background-color: hsl(0, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> However, this product failed to deliver where it's needed the most and that is the range.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I'm not sure why the last guy says it works from 40-50 feet but mine was only able to work within 10 feet.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I also had to point my presenter directly at the USB device to work.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> At this range, I might as well walk to the computer to change a slide.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Junk.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-1.37</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> What a giant cop-out.:                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Fans of the first five (or 4 1/2) seasons of Will & Grace who watched in horror as the series plummeted to its death LONG before the finale will not be surprised at the sheer inanity of the last episode.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 54%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It confirmed what we all knew: The writers ran out of ideas some time ago.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> To begin with, a 20-year estrangement was just NOT believable, given what we (used to) know about the characters.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 58%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> They should have done an \"Everybody Loves Raymond\"-type series finale, which didn't try to be \"cute\" but instead managed to neatly encapsulate all the major themes of the series without resorting to a bloated hour-long \"special\" with flashbacks, flashforwards, dreams, music (much as I enjoy Jack and Karen's abilities in that area) etc etc etc.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> W&G, sadly, outstayed their welcome.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Too bad, because in its prime the show was hilarious.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-3.22</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> A total waste of money - not recommended.:                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This feeder is made of plastic - the only metal on it is the hanger.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The feeding holes inserts are made of a soft plastic, and the birds and squirrels pecked and chewed through it within two days.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The product description states \"with plastic top and wooden perches\", but this is not so - everything is plastic.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The feeder that this one was to replace had metal inserts and a metal top on a plastic body, and lasted almost 10 years.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> In summary: the purchase of this feeder was a total waste of money - 100% junk!Heath Outdoor Products 392 Mixed Seed Feeder\n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-4.12</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Not up to O'Reilly standards: Bruce Tate's Beyond Java is not up to O'Reilly standards.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It's poorly organized, repetitive, and error-ridden.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> As of the present writing (Jan. 2, 2006), the online example code promised in the front of the book is not available on the O'Reilly web site.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> A long list of errata is available there, however, but it's incomplete.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Expect much frustration in trying to type in the main Ruby on Rails example and getting it to run successfully.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> And expect much hair-tearing at all the repetition and the author's attempt to be artsy by beginning each chapter by comparing white-water kayaking to the perils of picking a new programming language.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This would have made a good book or special journal issue at one-quarter it's present size, with better editing, and with better testing of the example code -- and deep-sixing the tedious kayak stories.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-1.70</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Doesn't make the grade: Harvey Mason is a drumkit wizard.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> A ubiquitous presence in the music industry for the past 40 some odd years his mark has been left on more records than you'd care to count.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> With an alluring title like \"Funk in a Mason Jar\" this one has always been on my radar but never readily avaiable.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Thanks to a new Japanese pressing it's here again.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> But it should either be left alone or sent to a museum somewhere for archiving.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If listless disco-funk and poppy soul is your thing and you were sporting an afro or mullett 30 years ago and wanna revisit old haunts, then go for it.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If you're a crate-digger type, don't be fooled by titles.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Funk in a Mason Jar is, at it's best high-minded disco and at it's worst low-brow funk and even lower as jazz.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Save your money - there's a reason this one was out of print for 20 years.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>4.09</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Beautiful!:                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This tape is great for anyone wanting some simple, effective self defence techniques for fighting larger attackers, or fighting in general.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This tape is stuffed full of them!                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Kicks, stomps, neck manipulations, knees, elbows, punches, bites, head butts, improvised weapons, you name it!                    </font></mark><mark style=\"background-color: hsl(120, 75%, 77%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The best part is seeing Bob whoop a guy that is 8 inches taller and outweighs him by 120 pounds!                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The only thing I disagree with is the closed fist punches to the head (good chance of broken knuckles), and head butting without controlling the opponent's head (good chance of cutting your scalp!).                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> But the good easily outweighs the bad.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> No one can argue with physics - size does matter.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> But Bob shows that the little guy still has a fighting chance.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 53%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> One of my favourite TRS tapes!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>3.35</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> PURE FUN!:                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> An album every music lover should own!Does Seattle have somthing in their water or what?                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> It starts rockin' with \"Kitty\" and,though each song is unique,keeps the same energy throughout the whole cd.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This is one of my favorite cds of all time.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> My top three of this album \"Naked and Famous\",\"Candy\" and \"Back Porch\"(which reminds me of the Darlings on The Andy Griffith Show...with a little grunge added of course).                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I wish the Presidents would come back because,like Stevie Ray,they left us wanting more.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.12)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-0.89</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Recipe of Spicy Sesame Linguine seems to have changed for worse & Price gone up by more than 50%: We have been ordering Spicy Sesame Linguine for the past 2-3 years as this was our favorite pasta!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> However, we recently realized that the taste of this linguine has lost its sesame flavor completely.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The pasta now looks very different in color and texture and from our experience has now become completely tasteless.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Moreover, the price of this pasta has increased from around $13.71 in 2010 (via Subscribe & Save) to $20.30 in July 2011, hence the price has ramped up MORE than 50%!What a pity the manufacturer of this pasta seem to have changed the recipe for worse.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> We were so disappointed that we even sent a complaint note on Amazon, who informed us that they have informed our disappointment to Al Dente.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> We hope that Al dente will make this pasta as delicious as it once used to be.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Thank you.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>True</b></text></td><td><text style=\"padding-right:2em\"><b>1.35</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Very enjoyable: This collection contains a variety of stories, so if all you're looking for is more \"Enchanted Forest\", then you may be disappointed.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Or you may be pleasantly suprised.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I think on the whole, though, the stories are suitable for an older, more mature reader than the EF books.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The recipe is potentially very good, though I found it difficult to spread the batter over the required size shield, er, pan, and found that it needed to be baked for only 20-25 minutes.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> My personal favourites were \"Forty Curses\", \"Cruel Sister\" and \"Utensile Strength.\"                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-1.18</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Sound Broke In Less Than 1 Week: I bought this for my 3 year old nephew's birthday.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> He liked it even though he had some trouble understanding how to use it.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> His 4 1/2 year old brother liked it better because he could figure out how to play the games.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Unfortunately, the sound broke after a couple of days, and their mom is going to try to return it to the store for a refund.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I saw that another reviewer had the sound go out as well, and other reviewers had problems with the mouse not working.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This toy is based on a fun concept, but it isn't very durable and may be better suited to ages 4 and up.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-1.24</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Usless as a real guide: This book was purchased a couple of years ago in the hope it would give me some help and insight into the world of the Ford Model A and how to \"bring it back\".                    </font></mark><mark style=\"background-color: hsl(0, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I thought it was poorly and amateurishly written then and after looking at it again last night, think it is even worse than that!!                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If you are working on an early 1928 car it may have some use.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The engine info is generalized and body information is worse - much is absolutely incorrect.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> In summation, THERE ARE A LOT BETTER SOURCES OF INFORMATION ON THIS VERY POPULAR AUTOMOBILE\n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-3.07</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Nice title, but way too basic info, too wordy (filler): I thought the title of the book was exactly what I was looking for.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I had a property to rent and thought about making it a vacation rental.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> So, I bought the book thinking it would help point out some not-so-obvious issues about renting a vacation property.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> But after reading it, I was disappointed with the lack of any real advice or tips.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> There are a A LOT of words, but they seem to be mostly filler with only very limited and basic useful information.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I would not recommend this book to anyone interested in running a real business.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-4.02</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Poor plot with poor acting: I REALLY WANTED TO SEE THIS SINCE IT WAS AN ED WOOD MOVIE.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I THOUGHT IT WAS RATHER ODD THE FIRST COUPLE OF MINUTES.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> WITH POOR DAY LIGHT TO NIGHT TIME INTERPRETATIONS, ORGY OF THE DEAD WAS DEFINITELY ONE OF THE LOWER FILMS ON THE LIST.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> IT HAS VERY POOR SCRIPT WRITING AND IT WAS SIMPLY DULL.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> YES THERE IS NUDITY BUT IT GETS OLD AS THE MOVIE DRAGS ON.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> IT WAS A VERY SLOW MOVING MOVIE.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I REALLY WOULD CONSIDER TO PASS THIS ONE UP, HOWEVER IF YOU STILL HAVE THE SLIGHTEST POINT OF INTEREST GO AHEAD AND RENT IT.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-2.09</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If only Mr. Berry could write: The Amber Room is my second and last visit to BerryLand.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Life is simply too short to spend it reading badly written books.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Mr. Berry is not a writer; if he were a chef, his skills would be strictly limited to microwave fare--fast, tasteless, and forgotten.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I am not being too hard on the \"writer\" since he has no claim to that title.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> His books are not just formulaic, but lacking in imagination and momentum--I'm surprised he hasn't addressed sunken treasure in the Bermuda Triangle yet (if, by chance, he has, please forgive my oversight).                    </font></mark><mark style=\"background-color: hsl(0, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Avoid this stuff if you value the hours of your life at all.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-1.28</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 63%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Not the Gong you think...: For those who think that Gong is defined by their wonderful and psychedelic Planet Gong trilogy, \"Gazeuse\" will prove a massive disappointment.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Coming after the departure of Daevid Allen, Steve Hillage, and others of the core lineup, the bulk of this album is made up of snore-o-delic jazz fusion with little spark, primarily featuring competent but somewhat uninspired guitar work by Allan Holdsworth and the ever-present remaining percussionist Pierre Moerlin.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Moerlin, it should be noted, turns in the saving grace of this album, the percussion workout 'Percolations'.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> But the lame jazz-fusion that makes up the bulk of this offering is little consolation.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Your money for Gong could be better-spent elsewhere...\n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.05)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-2.26</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> No Thank You, Sir.:                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I must admit that it was fun to read a bit of dirt on people as repugnant as Anne Coulter and Newt Gingrich.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> But the fun wore off when I realized I was just reading a left-wing version of the same sort of low-brow tripe that currently infects conservatism political \"thought\".                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Mr. Brock's epiphany has come conveniently late.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> He and his ilk have already mananged to dumb down the discourse on one side of the political aisle, and now it seems he'd like to expand the franchise to the Democratic side.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> No thank you, Mr. Brock.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> If I'd wanted to read books filled with pseudo-political drivel written by un-journalists of questionable ethics, I would've become a Republican.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.04)</b></text></td><td><text style=\"padding-right:2em\"><b>False</b></text></td><td><text style=\"padding-right:2em\"><b>-2.46</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Thought it couldn't get worse?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Guess again: This album reminds me of the maxim \"Fool me once, shame on you, fool me twice, shame on me.\"                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Having seen the radical departure the band took with its 1992 release, Aqua (featuring a new lead singer, drummer and guitarist), I wasn't quite sure what to expect from Aria.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Hugely disappointed in Aqua, I guess I hoped Geoff would return more to the sound that made the original Asia so popular - soaring guitar and keyboards, precision percussion, thundering bass, and powerful vocals.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Well, the soaring keyboards are still there, but that's about it.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> The songwriting is still lacking, and vocalist John Payne still plays the poseur more than the performer.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Were it not for the band's name, most people wouldn't give Aria a second thought.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Unfortunately for the band, those who do give it a listen probably won't give it a second thought, either.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> \n",
       "                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = visualization.visualize_text(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Finally, please remember to delete the Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

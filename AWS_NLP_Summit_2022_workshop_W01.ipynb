{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS NLP Summit 2022 - Hands-on Workshop W01 Hugging Face Model Explainability for customer sentiment text analytics using SageMaker Clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. [Overview](#Overview)\n",
    " 1. [Prerequisites and Data](#Prerequisites-and-Data)\n",
    "     1. [Initialize SageMaker](#Initialize-SageMaker)\n",
    "     1. [Data preparation for model training](#Data-preparation-for-model-training) \n",
    " 1. [Train and Deploy Hugging Face Model](#Train-and-Deploy-Hugging-Face-Model)\n",
    "     1. [Train model with Hugging Face estimator](#Train-model-with-Hugging-Face-estimator)\n",
    "     1. [Deploy Model to Endpoint](#Deploy-Model)\n",
    " 1. [Model Explainability with SageMaker Clarify for text features](#Model-Explainability-with-SageMaker-Clarify-for-text-features)\n",
    "     1. [Explaining Predictions](#Explaining-Predictions)\n",
    "     1. [Visualize local explanations](#Visualize-local-explanations)\n",
    "     1. [Clean Up](#Clean-Up)\n",
    "\n",
    "## Overview\n",
    "In the workshop, we will use Amazon Review data set from Kaggle to train Hugging face mode, and predict customer sentiments, after use Amazon SageMaker Clarify to explain the hugging face models, which can help you understand which sections of text are most important for the predictions made by your model. This functionality can be used to explain an individual local prediction. You can define the length of the text segment (tokens, phrases, sentences, paragraphs) to understand and visualize a modelâ€™s behavior at multiple levels of granularity.\n",
    "\n",
    "Traning and test dataset from Kaggle\n",
    "https://www.kaggle.com/datasets/bittlingmayer/amazonreviews\n",
    "\n",
    "Pre-requirments. \n",
    "\n",
    "1. Sign-in Kaggle account, \n",
    "2. Download new API Token in your local and upload into your notebook.Please refer the doc. https://github.com/Kaggle/kaggle-api\n",
    "\n",
    "In doing so, the notebook will first train a [Hugging Face model](https://huggingface.co/models) using the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) in the SageMaker Python SDK using training dataset, then use SageMaker Clarify to analyze a testing dataset in CSV format, and then visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "1. Download kaggle.json from your Kaggle account, and upload into your notebook\n",
    "2. Create ~/.kaggle, move kaggle.json into ~/.kaggle\n",
    "3. change the access permissions;\n",
    "4. Download the dataset from Kaggle into your notebook;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ~/nlp-summit-workshop/kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d nabamitachakraborty/amazon-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy dataset to S3\n",
    "#Install AWS Cli\n",
    "#In order to upload the file to S3 we need to install AWS Cli.\n",
    "\n",
    "!pip install awscli --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the file\n",
    "from zipfile import ZipFile\n",
    "with ZipFile('amazon-reviews.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install botocore --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sagemaker botocore boto3 awscli --upgrade\n",
    "# Pls ingore error of pip's dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Data\n",
    "\n",
    "We require the following AWS resources to be able to successfully run this notebook.\n",
    "1. Kernel: Python 3 (Data Science) kernel on SageMaker Studio or `conda_python3` kernel on notebook instances\n",
    "2. Instance type: Any GPU instance. For hugging face model training, we use `ml.p3.2xlarge`, for hugging face model endpoint, we use `ml.g4dn.xlarge`\n",
    "3. [SageMaker Python SDK](https://pypi.org/project/sagemaker/) version 2.70.0 or greater\n",
    "4. [Transformers](https://pypi.org/project/transformers/) > 4.6.1\n",
    "5. [Datasets](https://pypi.org/project/datasets/) > 1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --quiet install \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" \"captum\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the latest version of the SageMaker Python SDK, boto, and AWS CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Initialize SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data loading and pre-processing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import botocore\n",
    "import sagemaker\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, clarify\n",
    "from captum.attr import visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# SageMaker session bucket is used to upload the dataset, model and model training logs\n",
    "sess = sagemaker.Session()\n",
    "sess = sagemaker.Session(default_bucket=sess.default_bucket())\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-sagemaker-clarify-text\"\n",
    "\n",
    "# Define the IAM role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# SageMaker Clarify model directory name\n",
    "model_path = \"model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you change the value of `model_path` variable above, please be sure to update the `model_path` in [`code/inference.py`](./code/inference.py) script as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data: Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", index_col=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(frac=0.80)\n",
    "df = df.sample(n=100000)\n",
    "# Original dataset has 3.6M data entries, for 100k entries, ml.p3.2xlarge training duration is 38min, for 1 hrs workshop, you can consider sampling 50k data instances for practic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally, 1 means negative label, 2 means positive label, we change to 0 and 1\n",
    "df.loc[df['Label'] == 1, 'Label'] = 0\n",
    "df.loc[df['Label'] == 2, 'Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common approach for model evaluation is using the train/validation/test split. Although this approach can be very effective in general, it can result in misleading results and potentially fail when used on classification problems with a severe class imbalance. Instead, the technique must be modified to stratify the sampling by the class label as below. Stratification ensures that all classes are well represented across the train, validation and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Label\"\n",
    "cols = \"Review\"\n",
    "\n",
    "X = df[cols]\n",
    "y = df[target]\n",
    "\n",
    "# Data split: 11%(val) of the 90% (train and test) of the dataset ~ 10%; resulting in 80:10:10split\n",
    "test_dataset_size = 0.10\n",
    "val_dataset_size = 0.11\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Stratified train-val-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_dataset_size, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_dataset_size, stratify=y_train, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Dataset: train \",\n",
    "    X_train.shape,\n",
    "    y_train.shape,\n",
    "    y_train.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: validation \",\n",
    "    X_val.shape,\n",
    "    y_val.shape,\n",
    "    y_val.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "print(\n",
    "    \"Dataset: test \",\n",
    "    X_test.shape,\n",
    "    y_test.shape,\n",
    "    y_test.value_counts(dropna=False, normalize=True).to_dict(),\n",
    ")\n",
    "\n",
    "# Combine the independent columns with the label\n",
    "df_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "df_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "df_val = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split the dataset into train, test, and validation datasets. We use the train and validation datasets during training process, and run Clarify on the test dataset.\n",
    "\n",
    "In the cell below, we convert the Pandas DataFrames into Hugging Face Datasets for downstream modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload prepared dataset to the S3\n",
    "Here, we upload the prepared datasets to S3 buckets so that we can train the model with the Hugging Face Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 key prefix for the datasets\n",
    "s3_prefix = \"samples/datasets/amazon_reviews\"\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test\"\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step of the workflow, we use the [Hugging Face Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) to load the pre-trained `distilbert-base-uncased` model and fine-tune the model on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with Hugging Face estimator\n",
    "The hyperparameters defined below are parameters that are passed to the custom PyTorch code in [`scripts/train.py`](./scripts/train.py). The only required parameter is `model_name`. The other parameters like `epoch`, `train_batch_size` all have default values which can be overriden by setting their values here.\n",
    "\n",
    "Training duration will be 40min\n",
    "(Original dataset has 3.6M data entries, when sampling 100k entries, ml.p3.2xlarge training duration is 38min, for 1 hrs workshop, you can consider sampling 50k data instances for practic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters passed into the training job\n",
    "hyperparameters = {\"epochs\": 1, \"model_name\": \"distilbert-base-uncased\"}\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"scripts\",\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.6.1\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    py_version=\"py36\",\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained model files for model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {huggingface_estimator.model_data} model.tar.gz\n",
    "! mkdir -p {model_path}\n",
    "! tar -xvf model.tar.gz -C  {model_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Deploy Model\n",
    "We are going to use the trained model files along with the PyTorch Inference container to deploy the model to a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"hf_model.tar.gz\", mode=\"w:gz\") as archive:\n",
    "    archive.add(model_path, recursive=True)\n",
    "    archive.add(\"code/\")\n",
    "prefix = s3_prefix.split(\"/\")[-1]\n",
    "zipped_model_path = sess.upload_data(path=\"hf_model.tar.gz\", key_prefix=prefix + \"/hf-model-sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"amazon-reviews-model-{}\".format(\n",
    "    datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    ")\n",
    "endpoint_name = \"amazon-reviews-endpoint-{}\".format(\n",
    "    datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(\n",
    "    entry_point=\"inference.py\",\n",
    "    name=model_name,\n",
    "    model_data=zipped_model_path,\n",
    "    role=get_execution_role(),\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    ")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.g4dn.xlarge\", endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model endpoint\n",
    "Lets test the model endpoint to ensure that deployment was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence1 = \"Good Functions But Bad Range: I bought this for my classroom and used it a few times. I loved the fact that it has a mouse function and a laser pointer(though it is a very weak laser). However, this product failed to deliver where it's needed the most and that is the range. I'm not sure why the last guy says it works from 40-50 feet but mine was only able to work within 10 feet. I also had to point my presenter directly at the USB device to work. At this range, I might as well walk to the computer to change a slide. Junk.\"\n",
    "test_sentence2 = \"Love the color! very soft. unique look. can't wait to wear it this fall\"\n",
    "test_sentence3 = (\n",
    "    \"Beautiful! This tape is great for anyone wanting some simple, effective self defence techniques for fighting larger attackers, or fighting in general. This tape is stuffed full of them! Kicks, stomps, neck manipulations, knees, elbows, punches, bites, head butts, improvised weapons, you name it! The best part is seeing Bob whoop a guy that is 8 inches taller and outweighs him by 120 pounds! The only thing I disagree with is the closed fist punches to the head (good chance of broken knuckles), and head butting without controlling the opponent's head (good chance of cutting your scalp!). But the good easily outweighs the bad.No one can argue with physics - size does matter. But Bob shows that the little guy still has a fighting chance. One of my favourite TRS tapes!\n",
    "\"\n",
    ")\n",
    "test_sentence4 = \"Poor plot with poor acting. I REALLY WANTED TO SEE THIS SINCE IT WAS AN ED WOOD MOVIE. I THOUGHT IT WAS RATHER ODD THE FIRST COUPLE OF MINUTES. WITH POOR DAY LIGHT TO NIGHT TIME INTERPRETATIONS, ORGY OF THE DEAD WAS DEFINITELY ONE OF THE LOWER FILMS ON THE LIST. IT HAS VERY POOR SCRIPT WRITING AND IT WAS SIMPLY DULL. YES THERE IS NUDITY BUT IT GETS OLD AS THE MOVIE DRAGS ON. IT WAS A VERY SLOW MOVING MOVIE. I REALLY WOULD CONSIDER TO PASS THIS ONE UP, HOWEVER IF YOU STILL HAVE THE SLIGHTEST POINT OF INTEREST GO AHEAD AND RENT IT.\n",
    "\"\n",
    "\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name, sess)\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.CSVDeserializer()\n",
    "predictor.predict([[test_sentence1], [test_sentence2], [test_sentence3], [test_sentence4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability with SageMaker Clarify for text features\n",
    "\n",
    "Now that the model is ready, and we are able to get predictions, we are ready to get explanations for text data from Clarify processing job. For a detailed example that showcases how to use the Clarify processing job, please refer to [this example](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.ipynb). This example shows how to get explanations for text data from Clarify.\n",
    "\n",
    "In the cell below, we create the CSV file to pass on to the Clarify dataset. We are using 10 samples here to make it fast, but we can use entire dataset at a time. We are also filtering out any reviews with less than 500 characters as long reviews provide better visualization with `sentence` level granularity (When granularity is `sentence`, each sentence is a feature, and we need a few sentences per review for good visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"clarify_data.csv\"\n",
    "num_examples = 10\n",
    "\n",
    "df_test[\"len\"] = df_test[\"Review\"].apply(lambda ele: len(ele))\n",
    "\n",
    "df_test_clarify = pd.DataFrame(\n",
    "    df_test[df_test[\"len\"] > 500].sample(n=num_examples, random_state=RANDOM_STATE),\n",
    "    columns=[\"Review\"],\n",
    ")\n",
    "df_test_clarify.to_csv(file_path, header=True, index=False)\n",
    "df_test_clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Predictions\n",
    "There are expanding business needs and legislative regulations that require explanations of _why_ a model made the decision it did. SageMaker Clarify uses SHAP to explain the contribution that each input feature makes to the final decision.\n",
    "\n",
    "How does the Kernel SHAP algorithm work? Kernel SHAP algorithm is a local explanation method. That is, it explains each instance or row of the dataset at a time. To explain each instance, it perturbs the features values - that is, it changes the values of some features to a baseline (or non-informative) value, and then get predictions from the model for the perturbed samples. It does this for a number of times per instance (determined by the optional parameter `num_samples` in `SHAPConfig`), and computes the importance of each feature based on how the model prediction changed.\n",
    "\n",
    "We are now extending this functionality to text data. In order to be able to explain text, we need the `TextConfig`. The `TextConfig` is an optional parameter of `SHAPConfig`, which you need to provide if you need explanations for the text features in your dataset. `TextConfig` in turn requires three parameters:\n",
    "1. `granularity` (required): To explain text features, Clarify further breaks down text into smaller text units, and considers each such text unit as a feature. The parameter `granularity` informs the level to which Clarify will break down the text: `token`, `sentence`, or `paragraph` are the allowed values for `granularity`.\n",
    "2. `language` (required): the language of the text features. This is required to tokenize the text to break them down to their granular form.\n",
    "3. `max_top_tokens` (optional): the number of top token attributions that will be shown in the output (we need this because the size of vocabulary can be very big). This is an optional parameter, and defaults to 50.\n",
    "\n",
    "Kernel SHAP algorithm requires a baseline (also known as background dataset). In case of tabular features, the baseline value/s for a feature is ideally a non-informative or least informative value for that feature. However, for text feature, the baseline values must be the value you want to replace the individual text feature (token, sentence or paragraph) with. For instance, in the example below, we have chosen the baseline values for `review` as `<XXX>`, and `granularity` is `sentence`. Every time a sentence has to replaced in the perturbed inputs, we will replace it with `<XXX>`.\n",
    "\n",
    "\n",
    "If baseline is not provided, a baseline is calculated automatically by SageMaker Clarify using K-means or K-prototypes in the input dataset for tabular features. For text features, if baseline is not provided, the default replacement value will be the string `<PAD>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, instance_count=1, instance_type=\"ml.m5.2xlarge\", sagemaker_session=sess\n",
    ")\n",
    "\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "explainability_output_path = \"s3://{}/{}/clarify-text-explainability\".format(bucket, prefix)\n",
    "explainability_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=file_path,\n",
    "    s3_output_path=explainability_output_path,\n",
    "    headers=[\"Review\"],\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_config = clarify.SHAPConfig(\n",
    "    baseline=[[\"<XXX>\"]],\n",
    "    num_samples=1000,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True,\n",
    "    text_config=clarify.TextConfig(granularity=\"sentence\", language=\"english\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the clarify explainability job involves spinning up a processing job and a model endpoint which may take a few minutes.\n",
    "# After this you will see a progress bar for the SHAP computation.\n",
    "# The size of the dataset (num_examples) and the num_samples for shap will effect the running time.\n",
    "# The duration is around 18min\n",
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize local explanations\n",
    "\n",
    "We use Captum to visualize the feature importances computed by Clarify.\n",
    "First, lets load the local explanations. Local text explanations can be found in the analysis results folder in a file named `out.jsonl` in the `explanations_shap` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_feature_attributions_file = \"out.jsonl\"\n",
    "analysis_results = []\n",
    "analysis_result = sagemaker.s3.S3Downloader.download(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file,\n",
    "    local_path=\"./\",\n",
    ")\n",
    "\n",
    "shap_out = []\n",
    "file = sagemaker.s3.S3Downloader.read_file(\n",
    "    explainability_output_path + \"/explanations_shap/\" + local_feature_attributions_file\n",
    ")\n",
    "for line in file.split(\"\\n\"):\n",
    "    if line:\n",
    "        shap_out.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local explanations file is a JSON Lines file, that contains the explanation of one instance per row. Let's examine the output format of the explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(shap_out[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the highest level of this JSON Line, there are two keys: `explanations`, `join_source_value` (Not present here as we have not included a `joinsource` column in the input dataset). `explanations` contains a list of attributions for each feature in the dataset. In this case, we have a single element, because the input dataset also had a single feature. It also contains details like `feature_name`, `data_type` of the features (indicating whether Clarify inferred the column as numerical, categorical or text). Each token attribution also contains a `description` field that contains the token itself, and the starting index of the token in original input. This allows you to reconstruct the original sentence from the output as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, we create a list of attributions and a list of tokens for use in visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_dataset = [\n",
    "    np.array([attr[\"attribution\"][0] for attr in expl[\"explanations\"][0][\"attributions\"]])\n",
    "    for expl in shap_out\n",
    "]\n",
    "tokens_dataset = [\n",
    "    np.array(\n",
    "        [attr[\"description\"][\"partial_text\"] for attr in expl[\"explanations\"][0][\"attributions\"]]\n",
    "    )\n",
    "    for expl in shap_out\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain predictions as well so that they can be displayed alongside the feature attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict([t for t in df_test_clarify.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is a wrapper around the captum that helps produce visualizations for local explanations. It will\n",
    "# visualize the attributions for the tokens with red or green colors for negative and positive attributions.\n",
    "def visualization_record(\n",
    "    attributions,  # list of attributions for the tokens\n",
    "    text,  # list of tokens\n",
    "    pred,  # the prediction value obtained from the endpoint\n",
    "    delta,\n",
    "    true_label,  # the true label from the dataset\n",
    "    normalize=True,  # normalizes the attributions so that the max absolute value is 1. Yields stronger colors.\n",
    "    max_frac_to_show=0.05,  # what fraction of tokens to highlight, set to 1 for all.\n",
    "    match_to_pred=False,  # whether to limit highlights to red for negative predictions and green for positive ones.\n",
    "    # By enabling `match_to_pred` you show what tokens contribute to a high/low prediction not those that oppose it.\n",
    "):\n",
    "    if normalize:\n",
    "        attributions = attributions / max(max(attributions), max(-attributions))\n",
    "    if max_frac_to_show is not None and max_frac_to_show < 1:\n",
    "        num_show = int(max_frac_to_show * attributions.shape[0])\n",
    "        sal = attributions\n",
    "        if pred < 0.5:\n",
    "            sal = -sal\n",
    "        if not match_to_pred:\n",
    "            sal = np.abs(sal)\n",
    "        top_idxs = np.argsort(-sal)[:num_show]\n",
    "        mask = np.zeros_like(attributions)\n",
    "        mask[top_idxs] = 1\n",
    "        attributions = attributions * mask\n",
    "    return visualization.VisualizationDataRecord(\n",
    "        attributions,\n",
    "        pred,\n",
    "        int(pred > 0.5),\n",
    "        true_label,\n",
    "        attributions.sum() > 0,\n",
    "        attributions.sum(),\n",
    "        text,\n",
    "        delta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can customize the following display settings\n",
    "normalize = True\n",
    "max_frac_to_show = 1\n",
    "match_to_pred = False\n",
    "labels = test_dataset[\"Label\"][:num_examples]\n",
    "vis = []\n",
    "for attr, token, pred, label in zip(attributions_dataset, tokens_dataset, preds, labels):\n",
    "    vis.append(\n",
    "        visualization_record(\n",
    "            attr, token, float(pred[0]), 0.0, label, normalize, max_frac_to_show, match_to_pred\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we compiled the record we are finally ready to render the visualization.\n",
    "\n",
    "We see a row per review in the selected dataset. For each row we have the prediction, the label, and the highlighted text. Additionally, we show the total sum of attributions (as attribution score) and its label (as attribution label), which indicates whether it is greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualization.visualize_text(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Finally, please remember to delete the Amazon SageMaker endpoint to avoid charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
